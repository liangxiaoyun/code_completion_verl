algorithm:
    adv_estimator: grpo
data:
    train_files: /data_train/liangxiaoyun/datas/completion_sft_datas/train_data_merge_user_distillation/250530_250630_completion_v1_sp_training_0731_deduplication_0.75_5_deduplicated_001_less_8000.parquet
    val_files: /data_train/liangxiaoyun/datas/online_complete_data/compl-0719-v1.1/0724_online_2_20250717-20250719_full.parquet
    shuffle: True
    train_batch_size: 1024
    max_prompt_length: 8000
    max_response_length: 256
    filter_overlong_prompts: True
    truncation: 'error'
actor_rollout_ref:
    model:
        path: /data_fast/jiaruiyu/workstation/user_data_analysis/LLM_post_training/output/250530_250630_completion_v1_sp_training_0731_deduplication_0.75_5_50k
        use_remove_padding: True
        enable_gradient_checkpointing: True
    actor:
        optim.lr: 1e-6
        use_dynamic_bsz: True
        optim.lr_warmup_steps: 40
        ppo_mini_batch_size: 4096
        ppo_micro_batch_size_per_gpu: 256
        fsdp_config.param_offload: True
        fsdp_config.optimizer_offload: True
        use_kl_loss: True
        kl_loss_coef: 0.001
        kl_loss_type: low_var_kl
        entropy_coeff: 0
    rollout:
        log_prob_micro_batch_size_per_gpu: 256
        tensor_model_parallel_size: 1
        name: vllm
        gpu_memory_utilization: 0.6
        max_num_batched_tokens: 16384
        n: 5
    ref:
        log_prob_micro_batch_size_per_gpu: 256
        fsdp_config.param_offload: True
    algorithm:
        use_kl_in_reward: False
trainer:
    critic_warmup: 0
    project_name: 'grpo-v8.6.5_250811'
    experiment_name: 'deepseek_coder_7b_grpo_v8.6.5_20150813'
    n_gpus_per_node: 8
    nnodes: 1
    save_freq: 20
    test_freq: 20
    logger: ['console','wandb','tensorboard']
    default_local_dir: /data_large/liangxiaoyun/model_output/grpo-v8.6.5_250811
    total_epochs: 4
custom_reward_function:
    path: /data_train/liangxiaoyun/projects/verl/verl/utils/reward_score/code_completion.py